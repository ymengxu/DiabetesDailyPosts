{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd97c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from urllib.request import urlopen, Request\n",
    "#from harvest import extract_data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import json\n",
    "#import csv\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82 Safari/537.36\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1c8a2",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6223408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if can't load post data in one time, save the loaded list of dictionaries and return later \n",
    "def write_list_of_ngram_dicts(list_of_dicts, filename):\n",
    "    '''store a list of dictionaries into txt file'''\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for dic in list_of_dicts:\n",
    "            data=json.dumps(dic) \n",
    "            file.write(data)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c209e",
   "metadata": {},
   "source": [
    "# Get links to posts\n",
    "and number of replies to each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_links_replies = list()\n",
    "for p in range(1,1833):  # 1832 pages of posts\n",
    "    # extract posts on each page\n",
    "    URL = 'https://www.diabetesdaily.com/forum/forums/type-2-diabetes.14/page-'+str(p)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # find all posts\n",
    "    posts = soup.find_all('div', class_ = 'structItem-title')\n",
    "    # find the corresponding number of pages of each post \n",
    "    meta_info = soup.find_all('div', class_ = 'structItem-cell structItem-cell--meta')\n",
    "    for i in range(len(meta_info)):\n",
    "        link = 'https://www.diabetesdaily.com' + posts[i].find('a', href = True)['href']\n",
    "        num_pages = meta_info[i].find('dd').text\n",
    "        post_links_replies.append([link,num_pages])\n",
    "\n",
    "# save in a txt file: link + number of replies of each post \n",
    "with open(\"post_links_replies.txt\", \"w\") as f:\n",
    "    for post in post_links_replies:\n",
    "        for k in post:\n",
    "            f.write(k +\" \")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d098c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18315"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload post links from txt\n",
    "with open(\"post_links_replies.txt\") as f:\n",
    "    post_links_replies = f.readlines()\n",
    "post_links_replies = [p.strip(' \\n').split(' ') for p in post_links_replies]\n",
    "\n",
    "len(post_links_replies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae88de",
   "metadata": {},
   "source": [
    "# Get post content\n",
    "post_text, post_link, user_link, likes \n",
    "load post links first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4579731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload post links from txt\n",
    "with open(\"post_links_replies.txt\") as f:\n",
    "    post_links_replies = f.readlines()\n",
    "post_links_replies = [p.strip(' \\n').split(' ') for p in post_links_replies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "988bac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0e2748e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(post_links_replies)):\n",
    "    link = post_links_replies[i][0]\n",
    "    \n",
    "    # some posts are not in the type2diabetes section, will show a '-' for num_pages\n",
    "    try:\n",
    "        num_pages = math.ceil((int(post_links_replies[i][1])+1)/20)\n",
    "    except: # skip this post\n",
    "        continue\n",
    "        \n",
    "    post_data = list()\n",
    "    for page in range(num_pages):\n",
    "        url = link + 'page-' + str(page+1)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "        replies = soup.find_all('div', class_ = 'message-inner')\n",
    "        for r in replies:\n",
    "            r_data = dict()\n",
    "            r_data['post_text'] = r.find_all('div', 'bbWrapper')[0].text\n",
    "            r_data['post_link'] = r.find_all('li', class_ = 'u-concealed')[0].find_all('a', href = True)[0]['href']\n",
    "            try:\n",
    "                r_data['user_link'] = r.find_all('h4', class_ = 'message-name')[0].find_all('a', href = True)[0]['href']\n",
    "            except:\n",
    "                r_data['user_link'] = 'Anonymous'\n",
    "            # likes\n",
    "            try: \n",
    "                like_link = r.find_all('a', class_ = 'reactionsBar-link', href = True)[0]['href']\n",
    "                reaction_URL = 'https://www.diabetesdaily.com' + like_link\n",
    "                like_page = requests.get(reaction_URL)\n",
    "                like_soup = BeautifulSoup(like_page.content, \"html.parser\")\n",
    "                like_people = like_soup.find_all('a', class_ = 'username')\n",
    "                like_people_link = []\n",
    "                for p in like_people:\n",
    "                    like_people_link.append(p['href'])\n",
    "            except:\n",
    "                like_people_link = []\n",
    "            r_data['likes'] = like_people_link\n",
    "            post_data.append(r_data)\n",
    "\n",
    "    posts_data.append(post_data)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        # save for every 1000 posts\n",
    "        write_list_of_ngram_dicts(posts_data, 'post_data.txt')\n",
    "        \n",
    "# finally, save all posts\n",
    "write_list_of_ngram_dicts(posts_data, 'post_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9f93a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/forum/threads/i-need-to-stop-expecting-perfection.75746/post-721664\n",
      "7092\n",
      "Try index = 7096 in post_links_replies: \n",
      "['https://www.diabetesdaily.com/forum/threads/i-need-to-stop-expecting-perfection.75746/', '13']\n",
      "Next post:\n",
      "['https://www.diabetesdaily.com/forum/threads/is-a-steady-a1c-of-5-9-with-no-meds-better-than-a-lower-a1c-with-meds.72322/', '34']\n"
     ]
    }
   ],
   "source": [
    "# if interrupted during loading, find the latest retrieved post and continue\n",
    "# print(posts_data[-1][0]['post_link'])\n",
    "# print(len(posts_data))\n",
    "# idx = 7096\n",
    "# print('Try index = ' + str(idx) + ' in post_links_replies: ')\n",
    "# print(post_links_replies[idx])\n",
    "# print('Next post:')\n",
    "# print(post_links_replies[idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17aa273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18309"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('post_data.txt', 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "posts_data = [json.loads(d) for d in data]\n",
    "len(posts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7f31b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'post_text': \"I posed this in another tread but thought it ought to have a thread of its' own so people who need it can find it.  It's a Pharma Industry initiative and may be useful to some having difficulty with the cost of meds.\\n\\nhttps://www.pparx.org/Intro.php\\n\\nALSO, don't forget to check with the manfacturer of your specific medication to see if they have a program from which you may benefit.  They all have them, at some level of assistance, on some drugs.  It's not difficult to find out if you can catch a break from them.\",\n",
       "  'post_link': '/forum/threads/partnership-for-perscription-assistanca-ppa.86/post-347',\n",
       "  'user_link': '/forum/members/nikkisdad.101/',\n",
       "  'likes': []},\n",
       " {'post_text': 'Thanks for posting that! If any of you find it useful, let me know. I get regular emails asking about prescription assistance and it would be nice to have a few places to send them.',\n",
       "  'post_link': '/forum/threads/partnership-for-perscription-assistanca-ppa.86/post-349',\n",
       "  'user_link': '/forum/members/admin.1/',\n",
       "  'likes': []}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76371161",
   "metadata": {},
   "source": [
    "# Get user info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4e93dad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8431"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all distinct users\n",
    "users = set()\n",
    "for post in posts_data:\n",
    "    for reply in post:\n",
    "        users.add(reply['user_link'])\n",
    "        for liker in reply['likes']:\n",
    "            users.add(liker)\n",
    "\n",
    "users = list(users)\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "20eb8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6cb006e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape to find information of each user\n",
    "for i in range(len(users)):\n",
    "    user_data = dict()\n",
    "    \n",
    "    if users[i] == 'Anonymous':\n",
    "        continue\n",
    "    \n",
    "    user_url = 'https://www.diabetesdaily.com' + users[i]\n",
    "    r = requests.get(user_url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        user_data['username'] = soup.find_all('span', class_ = 'username')[0].text\n",
    "        user_data['userTitle'] = soup.find_all('span', class_ = 'userTitle')[0].text\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # date joined and date last seen\n",
    "    for k in soup.find_all('dl', class_ = 'pairs pairs--inline'):\n",
    "        tmp = k.text.strip('\\n')\n",
    "        tmp = re.sub(r'\\n{1,}', ':', tmp)\n",
    "        user_data[tmp.split(':')[0]] = tmp.split(':')[1]\n",
    "\n",
    "    # num_replies, activity score\n",
    "    for k in soup.find_all('div', class_ = 'pairJustifier')[0].find_all('dl'):\n",
    "        tmp = k.text.strip('\\n\\t')\n",
    "        tmp = re.sub(r'(\\n{1,}|\\t{1,})', ':', tmp)\n",
    "        tmp = re.sub(r':{1,} {0,}', ':', tmp)\n",
    "        user_data[tmp.split(':')[0]] = tmp.split(':')[1]\n",
    "\n",
    "    # scrape the subsection\n",
    "    # about: personal description, followers, followees\n",
    "    about_url = 'https://www.diabetesdaily.com' + users[i] + 'about'\n",
    "    r = requests.get(about_url)\n",
    "    soup_about = BeautifulSoup(r.content, \"html.parser\")\n",
    "    # description\n",
    "    # first section of description\n",
    "    try:\n",
    "        d1 = soup_about.find_all('div', class_ = 'bbWrapper')[0].text\n",
    "    except:\n",
    "        d1 = ''\n",
    "    # second section of description\n",
    "    try:\n",
    "        d2 = soup_about.find_all('dl', class_ = \"pairs pairs--columns pairs--fixedSmall pairs--customField\")[0].text.strip('\\n\\t')\n",
    "        d2 = re.sub(r'(\\n{1,}|\\t{1,})', ':', d2)\n",
    "        d2 = re.sub(r':{1,} {0,}', ':', d2)\n",
    "        d2 = d2.split(':')[1]\n",
    "    except: \n",
    "        d2 = ''\n",
    "    if d1 == '':\n",
    "        d = d2\n",
    "    elif d2 == '':\n",
    "        d = d1\n",
    "    elif (d1 == '') & (d2 == ''):\n",
    "        d = ''\n",
    "    else:\n",
    "        d = d1 + '; ' + d2\n",
    "    user_data['Description'] = d\n",
    "\n",
    "    # followers, followees\n",
    "    followees = []\n",
    "    followers = []\n",
    "    try:\n",
    "        for followee in soup_about.find_all('ul', class_ = \"listHeap\")[0].find_all('a', href = True):\n",
    "            followees.append(followee['href'])\n",
    "        user_data['followees'] = followees\n",
    "        for follower in soup_about.find_all('ul', class_ = \"listHeap\")[1].find_all('a', href = True):\n",
    "            followers.append(follower['href'])\n",
    "        user_data['followers'] = followers\n",
    "    except:\n",
    "        user_data['followees'] = followees\n",
    "        user_data['followers'] = followers\n",
    "    \n",
    "    users_data[users[i]] = user_data\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        # save for every 1000 users\n",
    "        exDict = {'exDict': users_data}\n",
    "        with open('users_data.txt', 'w') as file:\n",
    "             file.write(json.dumps(exDict)) # use `json.loads` to do the reverse\n",
    "        \n",
    "# finally, save all users\n",
    "exDict = {'exDict': users_data}\n",
    "with open('users_data.txt', 'w') as file:\n",
    "     file.write(json.dumps(exDict)) # use `json.loads` to do the reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "89432349",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('users_data.txt', 'r', encoding='utf-8') as file:\n",
    "     exDict = file.readlines()\n",
    "users_data = json.loads(exDict[0])['exDict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "281ebef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted users: 7646\n",
      "Number of total users: 8431\n"
     ]
    }
   ],
   "source": [
    "print('Number of extracted users: ' + str(len(users_data)))\n",
    "print('Number of total users: ' + str(len(users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20b125",
   "metadata": {},
   "source": [
    "## Fix unextracted users\n",
    "Some user data are not extracted:   \n",
    "- One possiblity is that they deleted their accounts\n",
    "- The other possibility is that (for some reason) we need login to see their personal profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_extracted = [k for k in users if k not in users_data] \n",
    "\n",
    "# first login using your username and password\n",
    "URL = 'https://www.diabetesdaily.com/go/login'\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Administrator\\Desktop\\Lab\\chromedriver_win32\\chromedriver\")\n",
    "driver.get(URL)\n",
    "\n",
    "username = driver.find_element_by_id(\"user_login\")\n",
    "password = driver.find_element_by_id(\"user_pass\")\n",
    "\n",
    "# enter your own username and password\n",
    "x = input('Enter your username for www.diabetesdaily.com: ')\n",
    "y = input('Enter your password for www.diabetesdaily.com: ')\n",
    "\n",
    "username.send_keys(x)\n",
    "password.send_keys(y)\n",
    "driver.find_element_by_id(\"wppb-submit\").click()  # can check chrome to see if login successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "82c48756",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data2 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "71828560",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 686: Can't find page.\n",
      "i = 687: Can't find page.\n",
      "i = 688: Can't find page.\n",
      "i = 690: Can't find page.\n",
      "i = 691: Can't find page.\n",
      "i = 700: Can't find page.\n",
      "i = 705: Can't find page.\n",
      "i = 709: Can't find page.\n",
      "i = 710: Can't find page.\n",
      "i = 720: Can't find page.\n",
      "i = 731: Can't find page.\n",
      "i = 745: Can't find page.\n",
      "i = 752: Can't find page.\n",
      "i = 772: Can't find page.\n",
      "i = 779: Can't find page.\n",
      "i = 780: Can't find page.\n",
      "i = 781: Can't find page.\n"
     ]
    }
   ],
   "source": [
    "# retrieve the same information from the rest users\n",
    "for i in range(len(not_extracted))[678:]:\n",
    "    \n",
    "    if not_extracted[i] == 'Anonymous':\n",
    "        continue\n",
    "        \n",
    "    user_url = 'https://www.diabetesdaily.com' + not_extracted[i]\n",
    "    driver.get(user_url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    user_data = dict()\n",
    "    try: \n",
    "        user_data['username'] = soup.find_all('span', class_ = 'is-stroked')[0].text\n",
    "        user_data['userTitle'] = soup.find_all('span', class_ = 'userTitle')[0].text\n",
    "    except: \n",
    "        if soup.find_all('h1', class_ = 'p-title-value')[0].text == 'Oops! We ran into some problems.': \n",
    "            print(\"i = \" + str(i) + \": Can't find page.\")\n",
    "        else:\n",
    "            print(\"i = \" + str(i) + \": Other issue.\")\n",
    "        continue\n",
    "            \n",
    "    # date joined, date last seen\n",
    "    for k in soup.find_all('dl', class_ = 'pairs pairs--inline'):\n",
    "        tmp = k.text.strip('\\n')\n",
    "        tmp = re.sub(r'\\n{1,}', ':', tmp)\n",
    "        user_data[tmp.split(':')[0]] = tmp.split(':')[1]\n",
    "\n",
    "    # num_replies, activity score\n",
    "    for k in soup.find_all('div', class_ = 'pairJustifier')[0].find_all('dl'):\n",
    "        tmp = k.text.strip('\\n\\t')\n",
    "        tmp = re.sub(r'(\\n{1,}|\\t{1,})', ':', tmp)\n",
    "        tmp = re.sub(r':{1,} {0,}', ':', tmp)\n",
    "        user_data[tmp.split(':')[0]] = tmp.split(':')[1]\n",
    "\n",
    "    # scrape the subsection\n",
    "    # about: personal description, followers, followees\n",
    "    about_url = 'https://www.diabetesdaily.com' + not_extracted[0] + 'about'\n",
    "    driver.get(about_url)\n",
    "    soup_about = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    # description\n",
    "    # first section of description\n",
    "    try:\n",
    "        d1 = soup_about.find_all('div', class_ = 'bbWrapper')[0].text\n",
    "    except:\n",
    "        d1 = ''\n",
    "    # second section of description\n",
    "    try:\n",
    "        d2 = soup_about.find_all('dl', class_ = \"pairs pairs--columns pairs--fixedSmall pairs--customField\")[0].text.strip('\\n\\t')\n",
    "        d2 = re.sub(r'(\\n{1,}|\\t{1,})', ':', d2)\n",
    "        d2 = re.sub(r':{1,} {0,}', ':', d2)\n",
    "        d2 = d2.split(':')[1]\n",
    "    except: \n",
    "        d2 = ''\n",
    "\n",
    "    if d2 == '':\n",
    "        d = d1\n",
    "    elif d2 == '':\n",
    "        d = d1\n",
    "    elif (d1 == '') & (d2 == ''):\n",
    "        d = ''\n",
    "    else:\n",
    "        d = d1 + '; ' + d2\n",
    "    user_data['Description'] = d\n",
    "\n",
    "    # followers, followees\n",
    "    followees = []\n",
    "    followers = []\n",
    "    try:\n",
    "        for followee in soup_about.find_all('ul', class_ = \"listHeap\")[0].find_all('a', href = True):\n",
    "            followees.append(followee['href'])\n",
    "        user_data['followees'] = followees\n",
    "        for follower in soup_about.find_all('ul', class_ = \"listHeap\")[1].find_all('a', href = True):\n",
    "            followers.append(follower['href'])\n",
    "        user_data['followers'] = followers\n",
    "    except:\n",
    "        user_data['followees'] = followees\n",
    "        user_data['followers'] = followers\n",
    "    \n",
    "    users_data2[not_extracted[i]] = user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4b59338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = {**users_data, **users_data2}\n",
    "# save\n",
    "exDict = {'exDict': users_data}\n",
    "with open('users_data.txt', 'w') as file:\n",
    "     file.write(json.dumps(exDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7a6ad",
   "metadata": {},
   "source": [
    "## Fix follower/followee number \n",
    "Some users have more than 12 followers or followees, which are need folded in their profile webpage. We need extra scraping for them.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77e75ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some users have more than 12 followers or followees, need extra scraping\n",
    "with open('users_data.txt') as file:\n",
    "     data = file.readlines()\n",
    "users_data = json.loads(data[0])['exDict']\n",
    "\n",
    "long_user = [user for user in users_data if (len(users_data[user]['followers']) == 12) | (len(users_data[user]['followees']) == 12)]\n",
    "len(long_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b045f45",
   "metadata": {},
   "source": [
    "To ensure that we can see everyone's profile, login first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "758cf2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your username for www.diabetesdaily.com: 18915993507@163.com\n",
      "Enter your password for www.diabetesdaily.com: Fcyxx1201..\n"
     ]
    }
   ],
   "source": [
    "# first login using your username and password\n",
    "URL = 'https://www.diabetesdaily.com/go/login'\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Administrator\\Desktop\\Lab\\chromedriver_win32\\chromedriver\")\n",
    "driver.get(URL)\n",
    "\n",
    "username = driver.find_element_by_id(\"user_login\")\n",
    "password = driver.find_element_by_id(\"user_pass\")\n",
    "\n",
    "# enter your own username and password\n",
    "x = input('Enter your username for www.diabetesdaily.com: ')\n",
    "y = input('Enter your password for www.diabetesdaily.com: ')\n",
    "\n",
    "username.send_keys(x)\n",
    "password.send_keys(y)\n",
    "driver.find_element_by_id(\"wppb-submit\").click()  # can check chrome to see if login successful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438653d",
   "metadata": {},
   "source": [
    "Then iterate through each user in the _long_user_ list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1e908e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(long_user)):\n",
    "    # find all the people the current user is following (followees)\n",
    "    if len(users_data[long_user[i]]['followees']) == 12:\n",
    "        followee = []\n",
    "        page = 1\n",
    "        hasNextPage = True\n",
    "        while hasNextPage:\n",
    "            user_url = 'https://www.diabetesdaily.com' + long_user[i] + 'following/page-' + str(page)\n",
    "            driver.get(user_url)\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            # find followee in the current page\n",
    "            for p in soup.find_all('h3', class_ = 'contentRow-header'):\n",
    "                followee.append(p.find('a', href = True)['href'])\n",
    "            # detect if there is a next page \n",
    "            if len(soup.find_all('span', class_ = 'block-footer-controls')) > 0:\n",
    "                page += 1\n",
    "            else:\n",
    "                hasNextPage = False\n",
    "\n",
    "    # find all the followers of the current user\n",
    "    if len(users_data[long_user[i]]['followers']) == 12:\n",
    "        follower = []\n",
    "        page = 1\n",
    "        hasNextPage = True\n",
    "        while hasNextPage:\n",
    "            user_url = 'https://www.diabetesdaily.com' + long_user[i] + 'followers/page-' + str(page)\n",
    "            driver.get(user_url)\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            # find followee in the current page\n",
    "            for p in soup.find_all('h3', class_ = 'contentRow-header'):\n",
    "                follower.append(p.find('a', href = True)['href'])\n",
    "            # detect if there is a next page \n",
    "            if len(soup.find_all('span', class_ = 'block-footer-controls')) > 0:\n",
    "                page += 1\n",
    "            else:\n",
    "                hasNextPage = False\n",
    "    \n",
    "    users_data[long_user[i]]['followees'] = followee\n",
    "    users_data[long_user[i]]['followers'] = follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4a2aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "exDict = {'exDict': users_data}\n",
    "with open('users_data.txt', 'w') as file:\n",
    "     file.write(json.dumps(exDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c624560",
   "metadata": {},
   "source": [
    "# Interaction matrix\n",
    "Store each interaction between a user $u$ and a post $i$, defined as $u$ replying to $i$, in a csv file.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda1c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open('post_data.txt', 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "posts_data = [json.loads(d) for d in data]\n",
    "\n",
    "with open('users_data.txt', 'r', encoding='utf-8') as file:\n",
    "     exDict = file.readlines()\n",
    "users_data = json.loads(exDict[0])['exDict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9301889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only save interactions of users that we have data\n",
    "interactions = []\n",
    "for i in range(len(posts_data)):\n",
    "    for reply in posts_data[i]:\n",
    "        if reply['user_link'] in users_data.keys():\n",
    "            interactions.append((i, reply['user_link']))\n",
    "\n",
    "df = pd.DataFrame(interactions)\n",
    "df.columns = ['post_id', 'user_link']\n",
    "df = df.value_counts().reset_index()\n",
    "df.columns = ['post_id', 'user_link', 'num_replies']\n",
    "\n",
    "# some users have no interaction data, need to include them \n",
    "exist_users = df['user_link'].unique()\n",
    "for user in users_data:\n",
    "    if user not in exist_users:\n",
    "        df = df.append(pd.DataFrame({'post_id': ['no_post'], 'user_link': user, 'num_replies': 0}), ignore_index = True)\n",
    "\n",
    "df.to_csv('user_network/interactions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fc28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
